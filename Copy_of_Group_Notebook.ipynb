{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Group_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CSdEhS5jKzkb",
        "ErP_ocaxK9FU",
        "HaF8v-UBTcxq",
        "NYDxvWrbIxxk",
        "0m29ahkxHHwP",
        "V9xCDp82lbg1",
        "1KZlfP8L2gOS",
        "4qTbYTt_0je9"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grendelaglaeca/NMA-Computational-Neuroscience/blob/main/Copy_of_Group_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQdcHTDzy2tP"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXIw61Dk-M5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573a83ba-6936-45dc-d549-00ff90e42b7a"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy.stats\n",
        "import statsmodels.stats.multitest as smt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "from networkx.algorithms import approximation as approx\n",
        "from google.colab import files\n",
        "import statistics as stats\n",
        "import pprint as pp\n",
        "import itertools\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCL-9lC2Kqst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff50a7f-74af-43cb-d072-7f7527e97ec8"
      },
      "source": [
        "# Necessary for visualization\n",
        "!pip install nilearn --quiet\n",
        "from nilearn import plotting, datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 10.0 MB 3.7 MB/s \n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nilearn/datasets/__init__.py:96: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
            "  \"Numpy arrays.\", FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eJOYQqgSMKV"
      },
      "source": [
        "#@title Figure settings\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSdEhS5jKzkb"
      },
      "source": [
        "# Basic parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL4crLoCzkzk"
      },
      "source": [
        "# The download cells will store the data in nested directories starting here:\n",
        "HCP_DIR = \"./hcp\"\n",
        "if not os.path.isdir(HCP_DIR):\n",
        "  os.mkdir(HCP_DIR)\n",
        "\n",
        "# The data shared for NMA projects is a subset of the full HCP dataset\n",
        "N_SUBJECTS = 339\n",
        "\n",
        "# The data have already been aggregated into ROIs from the Glasesr parcellation\n",
        "N_PARCELS = 360\n",
        "\n",
        "# The acquisition parameters for all tasks were identical\n",
        "TR = 0.72  # Time resolution, in sec\n",
        "\n",
        "# The parcels are matched across hemispheres with the same order\n",
        "HEMIS = [\"Right\", \"Left\"]\n",
        "\n",
        "# Each experiment was repeated multiple times in each subject\n",
        "N_RUNS_REST = 4\n",
        "N_RUNS_TASK = 2\n",
        "\n",
        "# Time series data are organized by experiment, with each experiment\n",
        "# having an LR and RL (phase-encode direction) acquistion\n",
        "BOLD_NAMES = [\n",
        "  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n",
        "  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
        "  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n",
        "  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n",
        "  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n",
        "  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n",
        "  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n",
        "  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n",
        "  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n",
        "]\n",
        "\n",
        "# You may want to limit the subjects used during code development.\n",
        "# This will use all subjects:\n",
        "subjects = range(N_SUBJECTS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErP_ocaxK9FU"
      },
      "source": [
        "# Downloading data\n",
        "\n",
        "The rest and task data are shared in different files, but they will unpack into the same directory structure.\n",
        "\n",
        "Each file is fairly large and will take some time to download. If you are focusing only on rest or task analyses, you may not want to download only that dataset.\n",
        "\n",
        "We also separately provide some potentially useful behavioral covariate information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Lh1SEyZ_Kdh"
      },
      "source": [
        "fname = \"hcp_rest.tgz\"\n",
        "if not os.path.exists(fname):\n",
        "  !wget -qO $fname https://osf.io/bqp7m/download/\n",
        "  !tar -xzf $fname -C $HCP_DIR --strip-components=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ubuyGN7siwI"
      },
      "source": [
        "fname = \"hcp_task.tgz\"\n",
        "if not os.path.exists(fname):\n",
        "  !wget -qO $fname https://osf.io/s4h8j/download/\n",
        "  !tar -xzf $fname -C $HCP_DIR --strip-components=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4EQ0Y--VqUg"
      },
      "source": [
        "fname = \"hcp_covariates.tgz\"\n",
        "if not os.path.exists(fname):\n",
        "  !wget -qO $fname https://osf.io/x5p4g/download/\n",
        "  !tar -xzf $fname -C $HCP_DIR --strip-components=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLWaBeoJM28-"
      },
      "source": [
        "fname = f\"{HCP_DIR}/atlas.npz\"\n",
        "if not os.path.exists(fname):\n",
        "  !wget -qO $fname https://osf.io/j5kuc/download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaF8v-UBTcxq"
      },
      "source": [
        "## Loading region information\n",
        "\n",
        "Downloading either dataset will create the `regions.npy` file, which contains the region name and network assignment for each parcel.\n",
        "\n",
        "Detailed information about the name used for each region is provided [in the Supplement](https://static-content.springer.com/esm/art%3A10.1038%2Fnature18933/MediaObjects/41586_2016_BFnature18933_MOESM330_ESM.pdf) to [Glasser et al. 2016](https://www.nature.com/articles/nature18933).\n",
        "\n",
        "Information about the network parcellation is provided in [Ji et al, 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6289683/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28q_GDlXOyMi"
      },
      "source": [
        "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
        "region_info = dict(\n",
        "    name=regions[0].tolist(),\n",
        "    network=regions[1],\n",
        "    myelin=regions[2].astype(np.float),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUmm7gjhNBVF"
      },
      "source": [
        "We also provide the [parcellation on the fsaverage5 surface](https://figshare.com/articles/HCP-MMP1_0_projected_on_fsaverage/3498446) and approximate MNI coordinates of each region, which can be useful for visualization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJTGGZDHNSBD"
      },
      "source": [
        "with np.load(f\"{HCP_DIR}/atlas.npz\") as dobj:\n",
        "  atlas = dict(**dobj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYDxvWrbIxxk"
      },
      "source": [
        "# Helper functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDFPnQ07MmEd"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnq7H5h_IxLi"
      },
      "source": [
        "def get_image_ids(name):\n",
        "  \"\"\"Get the 1-based image indices for runs in a given experiment.\n",
        "\n",
        "    Args:\n",
        "      name (str) : Name of experiment (\"rest\" or name of task) to load\n",
        "    Returns:\n",
        "      run_ids (list of int) : Numeric ID for experiment image files\n",
        "\n",
        "  \"\"\"\n",
        "  run_ids = [\n",
        "    i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code\n",
        "  ]\n",
        "  if not run_ids:\n",
        "    raise ValueError(f\"Found no data for '{name}''\")\n",
        "  return run_ids\n",
        "\n",
        "def load_timeseries(subject, name, runs=None, concat=True, remove_mean=True):\n",
        "  \"\"\"Load timeseries data for a single subject.\n",
        "  \n",
        "  Args:\n",
        "    subject (int): 0-based subject ID to load\n",
        "    name (str) : Name of experiment (\"rest\" or name of task) to load\n",
        "    run (None or int or list of ints): 0-based run(s) of the task to load,\n",
        "      or None to load all runs.\n",
        "    concat (bool) : If True, concatenate multiple runs in time\n",
        "    remove_mean (bool) : If True, subtract the parcel-wise mean\n",
        "\n",
        "  Returns\n",
        "    ts (n_parcel x n_tp array): Array of BOLD data values\n",
        "\n",
        "  \"\"\"\n",
        "  # Get the list relative 0-based index of runs to use\n",
        "  if runs is None:\n",
        "    runs = range(N_RUNS_REST) if name == \"rest\" else range(N_RUNS_TASK)\n",
        "  elif isinstance(runs, int):\n",
        "    runs = [runs]\n",
        "\n",
        "  # Get the first (1-based) run id for this experiment \n",
        "  offset = get_image_ids(name)[0]\n",
        "\n",
        "  # Load each run's data\n",
        "  bold_data = [\n",
        "      load_single_timeseries(subject, offset + run, remove_mean) for run in runs\n",
        "  ]\n",
        "\n",
        "  # Optionally concatenate in time\n",
        "  if concat:\n",
        "    bold_data = np.concatenate(bold_data, axis=-1)\n",
        "\n",
        "  return bold_data\n",
        "\n",
        "\n",
        "def load_single_timeseries(subject, bold_run, remove_mean=True):\n",
        "  \"\"\"Load timeseries data for a single subject and single run.\n",
        "  \n",
        "  Args:\n",
        "    subject (int): 0-based subject ID to load\n",
        "    bold_run (int): 1-based run index, across all tasks\n",
        "    remove_mean (bool): If True, subtract the parcel-wise mean\n",
        "\n",
        "  Returns\n",
        "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
        "\n",
        "  \"\"\"\n",
        "  bold_path = f\"{HCP_DIR}/subjects/{subject}/timeseries\"\n",
        "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
        "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
        "  if remove_mean:\n",
        "    ts -= ts.mean(axis=1, keepdims=True)\n",
        "  return ts\n",
        "\n",
        "def load_evs(subject, name, condition):\n",
        "  \"\"\"Load EV (explanatory variable) data for one task condition.\n",
        "\n",
        "  Args:\n",
        "    subject (int): 0-based subject ID to load\n",
        "    name (str) : Name of task\n",
        "    condition (str) : Name of condition\n",
        "\n",
        "  Returns\n",
        "    evs (list of dicts): A dictionary with the onset, duration, and amplitude\n",
        "      of the condition for each run.\n",
        "\n",
        "  \"\"\"\n",
        "  evs = []\n",
        "  for id in get_image_ids(name):\n",
        "    task_key = BOLD_NAMES[id - 1]\n",
        "    ev_file = f\"{HCP_DIR}/subjects/{subject}/EVs/{task_key}/{condition}.txt\"\n",
        "    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
        "    ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
        "    evs.append(ev)\n",
        "  return evs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQzCA99sMryW"
      },
      "source": [
        "## Task-based analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgnEuN0gMqxP"
      },
      "source": [
        "def condition_frames(run_evs, skip=0):\n",
        "  \"\"\"Identify timepoints corresponding to a given condition in each run.\n",
        "\n",
        "  Args:\n",
        "    run_evs (list of dicts) : Onset and duration of the event, per run\n",
        "    skip (int) : Ignore this many frames at the start of each trial, to account\n",
        "      for hemodynamic lag\n",
        "\n",
        "  Returns:\n",
        "    frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n",
        "\n",
        "  \"\"\"\n",
        "  frames_list = []\n",
        "  for ev in run_evs:\n",
        "\n",
        "    # Determine when trial starts, rounded down\n",
        "    start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
        "\n",
        "    # Use trial duration to determine how many frames to include for trial\n",
        "    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
        "\n",
        "    # Take the range of frames that correspond to this specific trial\n",
        "    frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n",
        "\n",
        "    frames_list.append(np.concatenate(frames))\n",
        "\n",
        "  return frames_list\n",
        "\n",
        "\n",
        "def selective_average(timeseries_data, ev, skip=0):\n",
        "  \"\"\"Take the temporal mean across frames for a given condition.\n",
        "\n",
        "  Args:\n",
        "    timeseries_data (array or list of arrays): n_parcel x n_tp arrays\n",
        "    ev (dict or list of dicts): Condition timing information\n",
        "    skip (int) : Ignore this many frames at the start of each trial, to account\n",
        "      for hemodynamic lag\n",
        "\n",
        "  Returns:\n",
        "    avg_data (1D array): Data averagted across selected image frames based\n",
        "    on condition timing\n",
        "\n",
        "  \"\"\"\n",
        "  # Ensure that we have lists of the same length\n",
        "  if not isinstance(timeseries_data, list):\n",
        "    timeseries_data = [timeseries_data]\n",
        "  if not isinstance(ev, list):\n",
        "    ev = [ev]\n",
        "  if len(timeseries_data) != len(ev):\n",
        "    raise ValueError(\"Length of `timeseries_data` and `ev` must match.\")\n",
        "\n",
        "  # Identify the indices of relevant frames\n",
        "  frames = condition_frames(ev, skip)\n",
        "\n",
        "  # Select the frames from each image\n",
        "  selected_data = []\n",
        "  for run_data, run_frames in zip(timeseries_data, frames):\n",
        "    run_frames = run_frames[run_frames < run_data.shape[1]]\n",
        "    selected_data.append(run_data[:, run_frames])\n",
        "\n",
        "  # Take the average in each parcel\n",
        "  avg_data = np.concatenate(selected_data, axis=-1).mean(axis=-1)\n",
        "\n",
        "  return avg_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m29ahkxHHwP"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Derive Connectome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wRwSEi77-02"
      },
      "source": [
        "from nilearn.connectome import sym_matrix_to_vec\n",
        "\n",
        "#Indices of brain networks of interest\n",
        "network_idx = np.where((region_info['network'] == 'Language'))# | (region_info['network'] == 'Cingulo-Oper'))[0]\n",
        "\n",
        "#Returns ROIxROI connectome \n",
        "arrays = []\n",
        "\n",
        "for i in range(2):\n",
        "  time_series = load_timeseries(subject=i, name=\"rest\", runs=1)\n",
        "  corr = np.corrcoef(time_series)\n",
        "  net_corr = corr[network_idx]\n",
        "  net_corr2 = net_corr.T[network_idx]\n",
        "  flat_corr = sym_matrix_to_vec(net_corr2, discard_diagonal=True)\n",
        "  arrays.append(flat_corr)\n",
        "\n",
        "#Dataframe of ROI correlation\n",
        "corr_df = pd.DataFrame(np.stack(arrays))\n",
        "\n",
        "#Returns whole brain flattened connectome\n",
        "whole_brain_arrays = []\n",
        "\n",
        "for i in range(1):\n",
        "  time_series = load_timeseries(subject=i, name=\"rest\", runs=1)\n",
        "  whole_brain_corr = np.corrcoef(time_series)\n",
        "  whole_brain_flat_corr = sym_matrix_to_vec(whole_brain_corr, discard_diagonal=True)\n",
        "  whole_brain_arrays.append(whole_brain_flat_corr)\n",
        "\n",
        "#Dataframe of whole brain correlation\n",
        "whole_brain_corr_df = pd.DataFrame(np.stack(whole_brain_arrays))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc2k_1BYk33Y"
      },
      "source": [
        "# **Network functions** *(bold chunks are the ones we developed and the main focus of the project)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMUpL7Fhvaei",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "300e275d-68ca-4ff9-9263-8bd9f410de7d"
      },
      "source": [
        "time_series_network = select_network('Language', 'Language', 1)\n",
        "windows = sliding_window(time_series_network, 30, 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f4c49cc738b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtime_series_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Language'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Language'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msliding_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_series_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'select_network' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-H6SKWviczs"
      },
      "source": [
        "def select_network(network, task, subject):\n",
        "  \"\"\"Gets data from regions belonging to the specified network from the specified task.\n",
        "  \"\"\"\n",
        "  # Select network regions.\n",
        "  time_series = load_timeseries(subject=subject, name=task) # Borrowed function.\n",
        "  network_indices = np.where((region_info['network'] == network)) # Requires region_info.\n",
        "  network_indices_list = list(network_indices[0])\n",
        "  time_series_network = time_series[network_indices_list,:]\n",
        "  # Select task moments.\n",
        "  ev = load_evs(subject, task, \"story\") ###################################################### SPECIFY THE TASK WITHIN THE RUN (\"math\", \"story\")\n",
        "  frames_list = condition_frames(ev, subject)\n",
        "  frames_list_flat = list(itertools.chain(*frames_list))\n",
        "  time_series_network = time_series_network[:, frames_list_flat]\n",
        "  return time_series_network\n",
        "\n",
        "\n",
        "def iter_corr_negative_only(time_series_network, threshold, binary):\n",
        "  \"\"\"Single dataset Spearman correlate region-wise, p-value filter and FDR correct.\n",
        "\n",
        "  Args:\n",
        "    time_series_network (array): n_parcel x n_tp arrays.\n",
        "    threshold (float): p-value significance level.\n",
        "    binary (bool): replace significant correlation coefficients with 1.\n",
        "\n",
        "  Returns:\n",
        "    correlation_matrix_corrected (dataframe): correlation matrix of regions x regions.\n",
        "  \"\"\"\n",
        "  # Correlate regions one by one (significance-sensitive).\n",
        "  correlation_matrix = pd.DataFrame(index=range(len(time_series_network)), columns=range(len(time_series_network)))\n",
        "  p_values = []\n",
        "  # nonsignificant_count = 0\n",
        "  for row1 in range(len(time_series_network)):\n",
        "    for row2 in range(row1 + 1, len(time_series_network)):\n",
        "      if all(v == 0 for v in time_series_network[row1]) or all(v == 0 for v in time_series_network[row2]):\n",
        "        correlation_matrix.at[row1, row2] = 0\n",
        "        p_values.append(1)\n",
        "      if all(v == 0 for v in time_series_network[row1]) == False and all(v == 0 for v in time_series_network[row2]) == False:\n",
        "        r, p = scipy.stats.spearmanr(time_series_network[row1], time_series_network[row2])\n",
        "        if p < threshold and r < 0:\n",
        "          if binary == True:\n",
        "            correlation_matrix.at[row1, row2] = 1\n",
        "          if binary == False:\n",
        "            correlation_matrix.at[row1, row2] = r\n",
        "        if p >= threshold or r >= 0:\n",
        "          correlation_matrix.at[row1, row2] = 0\n",
        "        # nonsignificant_count += 1\n",
        "        p_values.append(p)\n",
        "  correlation_matrix = correlation_matrix.astype(\"float64\")\n",
        "  # print(nonsignificant_count)\n",
        " \n",
        "  # FDR correction.\n",
        "  correlation_matrix_corrected = correlation_matrix.copy()\n",
        "  reject, pvals_corrected = smt.fdrcorrection(p_values, alpha=0.05, method='indep', is_sorted=False)\n",
        "  p_values_corrected = pd.DataFrame(index=range(len(time_series_network)), columns=range(len(time_series_network)))\n",
        "  # fdr_filtered_count = 0\n",
        "  item1 = 0\n",
        "  for row1 in range(len(time_series_network)):\n",
        "    for row2 in range(row1 + 1, len(time_series_network)):\n",
        "      if pvals_corrected[item1] >= threshold:\n",
        "        correlation_matrix_corrected.at[row1, row2] = 0\n",
        "        # fdr_filtered_count += 1\n",
        "      item1 += 1\n",
        "  # print(fdr_filtered_count-nonsignificant_count)\n",
        "\n",
        "  return correlation_matrix_corrected\n",
        "\n",
        "\n",
        "def iter_corr_positive_only(time_series_network, threshold, binary):\n",
        "  \"\"\"Single dataset Spearman correlate region-wise, p-value filter and FDR correct.\n",
        "\n",
        "  Args:\n",
        "    time_series_network (array): n_parcel x n_tp arrays.\n",
        "    threshold (float): p-value significance level.\n",
        "    binary (bool): replace significant correlation coefficients with 1.\n",
        "\n",
        "  Returns:\n",
        "    correlation_matrix_corrected (dataframe): correlation matrix of regions x regions.\n",
        "  \"\"\"\n",
        "  # Correlate regions one by one (significance-sensitive).\n",
        "  correlation_matrix = pd.DataFrame(index=range(len(time_series_network)), columns=range(len(time_series_network)))\n",
        "  p_values = []\n",
        "  # nonsignificant_count = 0\n",
        "  for row1 in range(len(time_series_network)):\n",
        "    for row2 in range(row1 + 1, len(time_series_network)):\n",
        "      if all(v == 0 for v in time_series_network[row1]) or all(v == 0 for v in time_series_network[row2]):\n",
        "        correlation_matrix.at[row1, row2] = 0\n",
        "        p_values.append(1)\n",
        "      if all(v == 0 for v in time_series_network[row1]) == False and all(v == 0 for v in time_series_network[row2]) == False:\n",
        "        r, p = scipy.stats.spearmanr(time_series_network[row1], time_series_network[row2])\n",
        "        if p < threshold and r > 0:\n",
        "          if binary == True:\n",
        "            correlation_matrix.at[row1, row2] = 1\n",
        "          if binary == False:\n",
        "            correlation_matrix.at[row1, row2] = r\n",
        "        if p >= threshold or r <= 0:\n",
        "          correlation_matrix.at[row1, row2] = 0\n",
        "        # nonsignificant_count += 1\n",
        "        p_values.append(p)\n",
        "  correlation_matrix = correlation_matrix.astype(\"float64\")\n",
        "  # print(nonsignificant_count)\n",
        " \n",
        "  # FDR correction.\n",
        "  correlation_matrix_corrected = correlation_matrix.copy()\n",
        "  reject, pvals_corrected = smt.fdrcorrection(p_values, alpha=0.05, method='indep', is_sorted=False)\n",
        "  p_values_corrected = pd.DataFrame(index=range(len(time_series_network)), columns=range(len(time_series_network)))\n",
        "  # fdr_filtered_count = 0\n",
        "  item1 = 0\n",
        "  for row1 in range(len(time_series_network)):\n",
        "    for row2 in range(row1 + 1, len(time_series_network)):\n",
        "      if pvals_corrected[item1] >= threshold:\n",
        "        correlation_matrix_corrected.at[row1, row2] = 0\n",
        "        # fdr_filtered_count += 1\n",
        "      item1 += 1\n",
        "  # print(fdr_filtered_count-nonsignificant_count)\n",
        "\n",
        "  return correlation_matrix_corrected\n",
        "\n",
        "\n",
        "def iter_corr(time_series_network, threshold, binary):\n",
        "  \"\"\"Single dataset Spearman correlate region-wise, p-value filter and FDR correct.\n",
        "\n",
        "  Args:\n",
        "    time_series_network (array): n_parcel x n_tp arrays.\n",
        "    threshold (float): p-value significance level.\n",
        "    binary (bool): replace significant correlation coefficients with 1.\n",
        "\n",
        "  Returns:\n",
        "    correlation_matrix_corrected (dataframe): correlation matrix of regions x regions.\n",
        "  \"\"\"\n",
        "  # Correlate regions one by one (significance-sensitive).\n",
        "  correlation_matrix = pd.DataFrame(index=range(len(time_series_network)), columns=range(len(time_series_network)))\n",
        "  p_values = []\n",
        "  # nonsignificant_count = 0\n",
        "  for row1 in range(len(time_series_network)):\n",
        "    for row2 in range(row1 + 1, len(time_series_network)):\n",
        "      if all(v == 0 for v in time_series_network[row1]) or all(v == 0 for v in time_series_network[row2]):\n",
        "        correlation_matrix.at[row1, row2] = 0\n",
        "        p_values.append(1)\n",
        "      if all(v == 0 for v in time_series_network[row1]) == False and all(v == 0 for v in time_series_network[row2]) == False:\n",
        "        r, p = scipy.stats.spearmanr(time_series_network[row1], time_series_network[row2])\n",
        "        if p < threshold:\n",
        "          if binary == True:\n",
        "            correlation_matrix.at[row1, row2] = 1\n",
        "          if binary == False:\n",
        "            correlation_matrix.at[row1, row2] = r\n",
        "        if p >= threshold:\n",
        "          correlation_matrix.at[row1, row2] = 0\n",
        "        # nonsignificant_count += 1\n",
        "        p_values.append(p)\n",
        "  correlation_matrix = correlation_matrix.astype(\"float64\")\n",
        "  # print(nonsignificant_count)\n",
        " \n",
        "  # FDR correction.\n",
        "  correlation_matrix_corrected = correlation_matrix.copy()\n",
        "  reject, pvals_corrected = smt.fdrcorrection(p_values, alpha=0.05, method='indep', is_sorted=False)\n",
        "  p_values_corrected = pd.DataFrame(index=range(len(time_series_network)), columns=range(len(time_series_network)))\n",
        "  # fdr_filtered_count = 0\n",
        "  item1 = 0\n",
        "  for row1 in range(len(time_series_network)):\n",
        "    for row2 in range(row1 + 1, len(time_series_network)):\n",
        "      if pvals_corrected[item1] >= threshold:\n",
        "        correlation_matrix_corrected.at[row1, row2] = 0\n",
        "        # fdr_filtered_count += 1\n",
        "      item1 += 1\n",
        "  # print(fdr_filtered_count-nonsignificant_count)\n",
        "\n",
        "  return correlation_matrix_corrected\n",
        "\n",
        "\n",
        "def sliding_window(time_series_network, window_size, step):\n",
        "  \"\"\"Chop an array (time series) into equally-sized subsets of contiguous\n",
        "     columns (windows) for every step.\n",
        "\n",
        "  Args:\n",
        "    time_series_network (array): n_parcel x n_tp arrays.\n",
        "    winsow_size (int.): amount of columns (time steps) of the window.\n",
        "    step (int.): step size for window placement.\n",
        "\n",
        "  Returns:\n",
        "    windows (list): ordered list of subsets (arrays) of contiguous columns\n",
        "    (windows) of an array (time series) of window_size size.\n",
        "  \"\"\"\n",
        "  windows = []\n",
        "  for i in range(0, (len(time_series_network[0]) - window_size)+1, step):\n",
        "    window = time_series_network[:, range(i, i + window_size)]\n",
        "    windows.append(window)\n",
        "\n",
        "  return windows\n",
        "\n",
        "\n",
        "def df_to_graph(corr_matrices):\n",
        "  \"\"\"Documentation pending but basically takes a list of correlation matrices\n",
        "  and turns each of them into a graph object through a number of bureaucratic\n",
        "  data type transformations, this function is hideous and I hate it. Pandas is\n",
        "  so much better than Numpy but NetworkX apparently thinks otherwise. End of rant.\n",
        "  \"\"\"\n",
        "  graphs = []\n",
        "  matrices = []\n",
        "  dataframes = []\n",
        "  for elem in corr_matrices:\n",
        "    array = elem.to_numpy()\n",
        "    np.nan_to_num(array, copy=False, nan=0.0, posinf=None, neginf=None)\n",
        "    new_array = array + array.T\n",
        "    for i in range(len(new_array)):\n",
        "     new_array[i][i] = 0\n",
        "    matrices.append(new_array)\n",
        "    graph = nx.Graph(new_array)\n",
        "    graphs.append(graph)\n",
        "    dataframe = pd.DataFrame(new_array)\n",
        "    dataframes.append(dataframe)\n",
        "\n",
        "  return graphs, matrices, dataframes\n",
        "\n",
        "\n",
        "def get_graphs(network, task, subject, threshold, window_size, step, binary):\n",
        "  \"\"\"Documentation pending but basically iters all previous functions, taking in\n",
        "  a time series and some basic parameters (see above) and returns a list of graphs.\n",
        "  \"\"\"\n",
        "  time_series_network = select_network(network, task, subject)\n",
        "  windows = sliding_window(time_series_network, window_size, step)\n",
        "  corr_matrices = []\n",
        "  for i in range(len(windows)):\n",
        "    correlation_matrix_corrected = iter_corr_positive_only(windows[i], threshold, binary)\n",
        "    corr_matrices.append(correlation_matrix_corrected)\n",
        "  graphs, matrices, dataframes = df_to_graph(corr_matrices)\n",
        "    \n",
        "  return graphs, matrices, dataframes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9xCDp82lbg1"
      },
      "source": [
        "# **Analysis functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0PrgVOkPoOG"
      },
      "source": [
        "windows[0][0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU6UENlKQNB5"
      },
      "source": [
        "len(windows[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG980FptRfp3"
      },
      "source": [
        "average_activity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sKv3GrPTR75"
      },
      "source": [
        "np.mean(windows[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIN18xmHT-5B"
      },
      "source": [
        "(windows[0].shape[0] * windows[0].shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7rl6j49Rq8g"
      },
      "source": [
        "np.sum(windows[0]) / (windows[0].shape[0] * windows[0].shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6csRHpiXKVRz"
      },
      "source": [
        "  # time_series_network = select_network(network, task, subject)\n",
        "  # windows = sliding_window(time_series_network, window_size, step)\n",
        "  average_activity = []\n",
        "  for window in windows:\n",
        "    sum = 0\n",
        "    for axis0 in range(len(window)):\n",
        "      for axis1 in range(len(window[0])):\n",
        "        sum =+ window[axis0][axis1]\n",
        "    mean = sum/(len(window)**2)\n",
        "    average_activity.append(mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_dnSEZmlXAe"
      },
      "source": [
        "def coincidence_ratio(dataframes):\n",
        "  \"\"\"Computes coincidence ratio (matching values/total values) between correlation\n",
        "  matrices. Naturally it performs poorly with weighted matrices.\n",
        "  \"\"\"\n",
        "  coincidences = pd.DataFrame(index=range(len(dataframes)), columns=range(len(dataframes)))\n",
        "  for dataframe1 in range(len(dataframes)):\n",
        "    for dataframe2 in range(dataframe1 + 1, len(dataframes)):\n",
        "      count = 0\n",
        "      total = 0\n",
        "      for region1 in range(len(dataframes[dataframe1])):\n",
        "        for region2 in range(region1 + 1, len(dataframes[dataframe1])):\n",
        "          if dataframes[dataframe1].at[region1, region2] == dataframes[dataframe2].at[region1, region2]:\n",
        "            count += 1\n",
        "            total += 1\n",
        "          if dataframes[dataframe1].at[region1, region2] != dataframes[dataframe2].at[region1, region2]:\n",
        "            total +=1\n",
        "      ratio = count / total\n",
        "      coincidences.at[dataframe1, dataframe2] = ratio\n",
        "\n",
        "  return coincidences\n",
        "\n",
        "\n",
        "def mean_signal(network, task, subject, window_size, step):\n",
        "  \"\"\"Averages all values in a window, for all windows.\n",
        "  \"\"\"\n",
        "  time_series_network = select_network(network, task, subject)\n",
        "  windows = sliding_window(time_series_network, window_size, step)\n",
        "  average_activity = []\n",
        "  for window in windows:\n",
        "    sum = 0\n",
        "    for axis0 in range(len(window)):\n",
        "      for axis1 in range(len(window[0])):\n",
        "        sum =+ window[axis0][axis1]\n",
        "    mean = sum/(len(window) * window_size)\n",
        "    average_activity.append(mean)\n",
        "\n",
        "  return average_activity\n",
        "\n",
        "\n",
        "def stats_trio(name, list_temp, parameters_names, parameters_list):\n",
        "  \"\"\"Computes the mean, standard deviation and coefficient of variation of the given parameters.\n",
        "  \"\"\"\n",
        "  parameters_names.append(name + \"_mean\")\n",
        "  #parameters_names.append(name + \"_standard_deviation\")\n",
        "  parameters_names.append(name + \"_coefficient_of_variation\")\n",
        "  if stats.mean(list_temp) != 0 and stats.stdev(list_temp) != 0:\n",
        "    parameters_list.append(stats.mean(list_temp))\n",
        "    #parameters_list.append(stats.stdev(list_temp))\n",
        "    parameters_list.append(stats.stdev(list_temp)/stats.mean(list_temp))\n",
        "  if stats.mean(list_temp) == 0 and stats.stdev(list_temp) != 0:\n",
        "    parameters_list.append(0)\n",
        "    #parameters_list.append(stats.stdev(list_temp))\n",
        "    parameters_list.append(stats.stdev(list_temp))\n",
        "  if stats.mean(list_temp) == 0 and stats.stdev(list_temp) == 0:\n",
        "    parameters_list.append(0)\n",
        "    #parameters_list.append(0)\n",
        "    parameters_list.append(0)\n",
        "  if stats.mean(list_temp) != 0 and stats.stdev(list_temp) == 0:\n",
        "    parameters_list.append(stats.mean(list_temp))\n",
        "    #parameters_list.append(0)\n",
        "    parameters_list.append(0)\n",
        "\n",
        "\n",
        "def graph_analysis(graph):\n",
        "  \"\"\"Computes a series of graph theoretical parameters (and creates a lable list).\n",
        "  \"\"\"\n",
        "  parameters_names = []\n",
        "  parameters_list = []\n",
        "  parameters_names.append(\"Average_clustering_coefficient\") # Average of: fraction of existing triangles of a node over all possibles, or, fraction of existing edges between adjacent nodes of a node over all possibles.\n",
        "  parameters_list.append(approx.average_clustering(graph))\n",
        "  #parameters_names.append(\"Minimum_maximal_matching_size\") # Number of nodes in the smallest (minimum) matching (groupd of non-adjacent nodes (no shared immediate neighbours)) including all nodes it possibly can (maximal).\n",
        "  #parameters_list.append(len(approx.min_maximal_matching(graph)))\n",
        "  ##parameters_names.append(\"Degree_assortativity_coefficient\")\n",
        "  ##parameters_list.append(nx.degree_assortativity_coefficient(graph))\n",
        "  list_temp = list(dict.values(nx.average_neighbor_degree(graph, source = \"out\", target = \"in\"))) # FIXME what are source and target attributes? They impact on results.\n",
        "  stats_trio(\"Average_neighbor_degree\", list_temp, parameters_names, parameters_list)\n",
        "  ##parameters_names.append(\"Minimum_edge_cover\") # Minimum number of the edges of a graph needed so that every node has at least one edge, meant for bipartite graphs but I believe it doesn't matter though.\n",
        "  ##parameters_list.append(len(nx.min_edge_cover(graph)))\n",
        "  #list_temp = list(dict.values(nx.degree_centrality(graph)))\n",
        "  #stats_trio(\"Degree_centrality\", list_temp, parameters_names, parameters_list)\n",
        "  ##list_temp = list(dict.values(nx.eigenvector_centrality(graph)))\n",
        "  ##stats_trio(\"Eigenvector_centrality\", list_temp, parameters_names, parameters_list) # Measures connectivity (degree?) of neighbors.\n",
        "  ##list_temp = list(dict.values(nx.katz_centrality(graph)))\n",
        "  ##stats_trio(\"Katz_centrality\", list_temp, parameters_names, parameters_list) # Measures distance to all reachable nodes with distance attenuation (?).\n",
        "  list_temp = list(dict.values(nx.closeness_centrality(graph)))\n",
        "  stats_trio(\"Closeness_centrality\", list_temp, parameters_names, parameters_list) # Average of: shortest path between a pair of nodes.\n",
        "  list_temp = list(dict.values(nx.betweenness_centrality(graph)))\n",
        "  stats_trio(\"Betweenness_centrality\", list_temp, parameters_names, parameters_list) # Averag of: ratio of shortests paths that use a node.\n",
        "  ##list_temp = list(dict.values(nx.edge_betweenness_centrality(graph)))\n",
        "  ##stats_trio(\"Edge_betweenness_centrality\", list_temp, parameters_names, parameters_list) # Averag of: ratio of shortests paths that use an edge (confirm).\n",
        "  list_temp = []\n",
        "  for i in nx.degree(graph):\n",
        "\t  list_temp.append(i[1])\n",
        "  stats_trio(\"Degree\", list_temp, parameters_names, parameters_list) # Degree.\n",
        "  parameters_names.append(\"Density\") # Fraction of existing edges over all possible edges.\n",
        "  parameters_list.append(nx.density(graph))\n",
        "  #list_temp = [] \n",
        "  #for i in list(nx.nodes(graph)):\n",
        "\t#  list_temp.append(len((list(nx.non_neighbors(graph, i)))))\n",
        "  #stats_trio(\"Non-neighbors\", list_temp, parameters_names, parameters_list)\n",
        "  #parameters_names.append(\"Edge_count\")\n",
        "  #parameters_list.append(nx.number_of_edges(graph))\n",
        "  ##parameters_names.append(\"Small_world_coefficient\")\n",
        "  ##parameters_list.append(nx.algorithms.smallworld.sigma(graph))\n",
        "\n",
        "  return parameters_names, parameters_list\n",
        "\n",
        "\n",
        "def iter_graph_analysis(graphs, network, task, subject, window_size, step, threshold2, binary2):\n",
        "  \"\"\"Takes the graph_analysis functions and performs it over all graphs in an\n",
        "  ordered way, then correlates all parameter timeseries between themselves and\n",
        "  and returns the corresponding matrix. I am ashamed of how unelegant this\n",
        "  function is (it was past midnight), I apologize for it and promise to fix it.\n",
        "  \"\"\"\n",
        "  parameters_names, parameters_list = graph_analysis(graphs[0])\n",
        "  for elem in parameters_names:\n",
        "    vars()[str(elem + \"_timeseries\")] = []\n",
        "  for graph in graphs:\n",
        "    parameters_names, parameters_list = graph_analysis(graph)\n",
        "    for i in range(len(parameters_list)):\n",
        "      vars()[str(parameters_names[i] + \"_timeseries\")].append(parameters_list[i])\n",
        "  all_parameters_array = []\n",
        "  for elem in parameters_names:\n",
        "    all_parameters_array.append(vars()[str(elem + \"_timeseries\")])\n",
        "  average_activation = mean_signal(network, task, subject, window_size, step)\n",
        "  all_parameters_array.append(average_activation)\n",
        "  parameters_names.append(\"Average_activation_timeseries\")\n",
        "  parameters_corr_array = iter_corr_negative_only(all_parameters_array, threshold2, binary2)\n",
        "  parameters_corr = pd.DataFrame(parameters_corr_array)\n",
        "  parameters_corr.index = parameters_names\n",
        "  parameters_corr.columns = parameters_names\n",
        "\n",
        "  return parameters_corr, parameters_names, all_parameters_array\n",
        "\n",
        "# Ok last function, I promise. This just puts everything together but it still deals with a single subject.\n",
        "def activity_parameters_corr(network, task, subject, threshold, threshold2, window_size, step, binary, binary2):\n",
        "\n",
        "  # Call task data from network regions and create dynamic graphs.\n",
        "  graphs, matrices, dataframes = get_graphs(network, task, subject, threshold, window_size, step, binary)\n",
        "\n",
        "  # Compute dynamic average activation.\n",
        "  average_activity = mean_signal(network, task, subject, window_size, step)\n",
        "\n",
        "  # Compute dynamic graph theoretical parameters.\n",
        "  parameters_corr, parameters_names, all_parameters_array = iter_graph_analysis(graphs, network, task, subject, window_size, step, threshold2, binary2)\n",
        "\n",
        "  # Compute dynamic binary similarity between graphs. I guess we won't be using this. Might be usefull for clustering...? (@Matt)\n",
        "  coincidences = coincidence_ratio(dataframes)\n",
        "\n",
        "  return parameters_corr, graphs, all_parameters_array, parameters_names\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPXvwb8hwcWp"
      },
      "source": [
        "# **Iterative functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjRrp11fapAZ"
      },
      "source": [
        "# This bit was super improvised and is very disorganized so if it seems random, redundant or difficult to comprehend, it's my fault.\n",
        "\n",
        "# Now we grab what we defined before and loop it over all subjects (or some of them).\n",
        "def iter_over_subjects(network, subjects, task, threshold, threshold2, window_size, step, binary, binary2):\n",
        "  parameters_corr_all_subjects = pd.DataFrame()\n",
        "  CVdf_crop_all_subjects = pd.DataFrame()\n",
        "  for subject in range(subjects): # if you want to do all subjects: \" for subjects in range(len(N_SUBJECTS)): \"\n",
        "    try:\n",
        "      # The goal of this small chunk is to store in one array the correlations between each graph metric and average activity only, for every subject.\n",
        "      # We run the full analysis on one subject at a time, we save parameters_corr (a correlation matrix between ALL metrics), graphs (the networks (one per window)) and all_parameters_array (a table of all metrics (23 I believe) across time (so one value per window, per metric)).\n",
        "      parameters_corr, graphs, all_parameters_array, parameters_names = activity_parameters_corr(network, task, subject, threshold, threshold2, window_size, step, binary, binary2)\n",
        "      # parameters_corr_all_subjects will keep track of correlations between all metrics and average activation only (of all subjects), for this we grab a subset of the main correlation matrix with [:-1] (the las column).\n",
        "      parameters_corr_all_subjects[subject] = parameters_corr[\"Average_activation_timeseries\"][:-1]\n",
        "      # Not important: since it takes some time to compute, the function lets you know when each subject is ready.\n",
        "      print(str(\"Done with subject nº \" + str(subject)))\n",
        "      \n",
        "      # The goal behind this chunk is to evaluate how much graph metrics evolve over time (with the standard deviation normalized by the mean (\"coefficient of variation\" or \"CV\")).\n",
        "      # Once again I'm sorry for the ugly code, here we turn the array with all the parameters of a subject into a dataframe just because I prefer dataframes (now is when I get lazy with variable names).\n",
        "      CVdf = pd.DataFrame(all_parameters_array)\n",
        "      # We give names to the rows of the dataframe so we know what metrics we're looking at.\n",
        "      CVdf.index = parameters_names\n",
        "      # We add a couple extra columns storing some basic statistics of every metric across time.\n",
        "      CVdf[\"mean\"] = CVdf.mean(axis=1) # Average metrics along the time series.\n",
        "      CVdf[\"SD\"] = CVdf.std(axis=1) # You get the idea.\n",
        "      CVdf[\"CV\"] = abs(CVdf.std(axis=1)/CVdf.mean(axis=1)) # This is the one we care about, how each metric varies across time, scaled for magnitude.\n",
        "      # We pick this last column and get rid of the rest to declutter (specially since we will be appending for all subjects).\n",
        "      CVdf_crop = CVdf[\"CV\"]\n",
        "      # We store this data of each subject in a big dataframe of all subjects.\n",
        "      CVdf_crop_all_subjects[subject] = CVdf_crop\n",
        "    except:\n",
        "      print(subject)\n",
        "\n",
        "  return parameters_corr_all_subjects, CVdf_crop_all_subjects\n",
        "\n",
        "# Ok so to wrap things up (again sorry for the mess): we ended up with two beautiful dataframes: parameters_corr_all_subjects and CVdf_crop_all_subjects.\n",
        "# These dataframes follow the same logic in that both list graph metrics as rows and subjects as columns, the difference between the two is that\n",
        "# parameters_corr_all_subjects stores the CORRELATION of each metric with the average activity, and CVdf_crop_all_subjects stores the COEFFICIENT OF VARIATION\n",
        "# of each metric across time.\n",
        "\n",
        "# This becomes much clearer once you see the dataframes (visualization is better if we turn them into strings, for some reason), hence the following lines.\n",
        "# Remember that columns are subjects (which we still need to summarize because we can't interpret 339 results at once (e.g., sum, average, etc.)) and rows are graph metrics.\n",
        "\n",
        "def get_results(parameters_corr_all_subjects, CVdf_crop_all_subjects):\n",
        "  parameters_corr_all_subjects_sum = parameters_corr_all_subjects.sum(axis=1)\n",
        "  #parameters_corr_all_subjects_sum.plot.barh()\n",
        "  CVdf_crop_all_subjects_mean = CVdf_crop_all_subjects.mean(axis=1)\n",
        "  CVdf_crop_all_subjects_mean_crop = CVdf_crop_all_subjects_mean.copy()\n",
        "  CVdf_crop_all_subjects_mean_crop = CVdf_crop_all_subjects_mean_crop.drop(\"Average_activation_timeseries\", 0)\n",
        "  #CVdf_crop_all_subjects_mean_crop.plot.barh()\n",
        "\n",
        "  return parameters_corr_all_subjects_sum, CVdf_crop_all_subjects_mean_crop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AMkeVN2YRNy"
      },
      "source": [
        "# **Get the graphs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D02iX_Gdq4w6"
      },
      "source": [
        "# Here we (finally) permorm the actual analysis, for this we have to specify the following parameters:\n",
        "network =      \"Language\"\n",
        "task =         \"Language\"\n",
        "subjects =     20\n",
        "threshold =    0.05 # of the between-region correlations for creating the correlation matrix.\n",
        "threshold2 =   0.05 # of the average activity-graph metric correaltions.\n",
        "window_size =  30 # Minimum according to Apoorva.\n",
        "step =         30 # Apoorva also suggested non-overlapping to avoid intrinsic correlation.\n",
        "binary =       True # of the between-region correlations for creating the correlation matrix.\n",
        "binary2 =      True # of the average activity-graph metric correaltions.\n",
        "\n",
        "parameters_corr_all_subjects, CVdf_crop_all_subjects = iter_over_subjects(network, subjects, task, threshold, threshold2, window_size, step, binary, binary2)\n",
        "parameters_corr_all_subjects_sum, CVdf_crop_all_subjects_mean_crop = get_results(parameters_corr_all_subjects, CVdf_crop_all_subjects)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv3Xl-aHh97a"
      },
      "source": [
        "print(parameters_corr_all_subjects_sum)\n",
        "parameters_corr_all_subjects_sum = np.flip(parameters_corr_all_subjects_sum)\n",
        "parameters_corr_all_subjects_sum.plot.barh()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGUEZi-8iThy"
      },
      "source": [
        "print(CVdf_crop_all_subjects_mean_crop)\n",
        "CVdf_crop_all_subjects_mean_crop = np.flip(CVdf_crop_all_subjects_mean_crop)\n",
        "CVdf_crop_all_subjects_mean_crop.plot.barh()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd_6yW41YWqL"
      },
      "source": [
        "# Other stuff (ignore)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvyYjE93wLVE"
      },
      "source": [
        "#CVdf_crop_all_subjects_mean_crop.plot.barh()\n",
        "\n",
        "s1 = parameters_corr_all_subjects.to_string()\n",
        "s2 = CVdf_crop_all_subjects.to_string()\n",
        "print(\" \")\n",
        "print(\"Correlations between each metric and average activation for each subject:\")\n",
        "print(s1)\n",
        "print(\" \")\n",
        "print(\"Coefficient of varation of each metric across time for each subject:\")\n",
        "print(s2)\n",
        "\n",
        "# Distribution \n",
        "\n",
        "sns.displot(listY, kind=\"kde\")\n",
        "\n",
        "# Joint plot for selected metric correlations.\n",
        "\n",
        "listX = CVdf_crop_all_subjects.loc[\"Average_activation_timeseries\"]\n",
        "listY = CVdf_crop_all_subjects.loc[\"Average_clustering_coefficient\"]\n",
        "sns.jointplot(listX, listY, kind=\"reg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLih-2G-rCSW"
      },
      "source": [
        "# parameters_corr_all_subjects\n",
        "# CVdf_crop_all_subjects\n",
        "  \n",
        "CVdf_crop_all_subjects[\"mean\"] = CVdf_crop_all_subjects.mean(axis=1)\n",
        "CVdf_crop_all_subjects[\"SD\"] = CVdf_crop_all_subjects.std(axis=1)\n",
        "CVdf_crop_all_subjects[\"CV\"] = abs(CVdf_crop_all_subjects.std(axis=1)/CVdf_crop_all_subjects.mean(axis=1))\n",
        "\n",
        "print(CVdf_crop_all_subjects[\"CV\"])\n",
        "print(parameters_corr_all_subjects)\n",
        "\n",
        "parameters_corr_all_subjects[\"sum\"] = parameters_corr_all_subjects.sum(axis=1)\n",
        "print(parameters_corr_all_subjects[\"sum\"])\n",
        "sns.barplot(parameters_corr_all_subjects[\"sum\"])\n",
        "\n",
        "# Get activity-metric correlation frecuency across all subjects.\n",
        "\n",
        "parameters_corr_all_subjects[\"sum\"].plot.barh()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KZlfP8L2gOS"
      },
      "source": [
        "# **Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFjaZIHY5-W_"
      },
      "source": [
        "# The most extra part of the script.\n",
        "# Don't plot both figures at once (or do so and see what happens). \n",
        "\n",
        "nx.draw(graphs[8]) # There are several specific plotting algorithms (circular, force, etc) that you can learn more about here: https://networkx.org/documentation/stable/reference/drawing.html?highlight=draw\n",
        "heatmap = sns.heatmap(parameters_corr, xticklabels=True, yticklabels=True) # The long labels ruin everything."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qTbYTt_0je9"
      },
      "source": [
        "# Other stuff that came with the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3Qtf5gxeGw4"
      },
      "source": [
        "#Plot connectivity matrix\n",
        "import numpy as np\n",
        "from nilearn import plotting\n",
        "from nilearn.connectome import ConnectivityMeasure\n",
        "\n",
        "#Generate pseudo labels for now\n",
        "labels = []\n",
        "for i in range(360):\n",
        "  labels.append('label' + str(i))\n",
        "\n",
        "# Mask the main diagonal for visualization:\n",
        "np.fill_diagonal(whole_brain_corr, 0)\n",
        "\n",
        "# Plot corr matrix\n",
        "plotting.plot_matrix(correlation_matrix, figure=(12, 10), labels=labels,\n",
        "                     vmax=0.8, vmin=-0.8, reorder=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}